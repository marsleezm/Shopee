{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-wheat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d40925e2fc69>:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth['f1'] = f1_score(ground_truth['matches'], ground_truth['posting_id'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the same posting id as prediction our f1 score is 0.4831068224713004\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "16809984/16804768 [==============================] - 2s 0us/step\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from scipy import spatial\n",
    "from tqdm.notebook import tqdm\n",
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = [384, 384]\n",
    "# Seed\n",
    "SEED = 42\n",
    "# Learning rate\n",
    "LR = 0.001\n",
    "# Verbosity\n",
    "VERBOSE = 2\n",
    "# Function to get our f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "# Function to read and preprocess our data\n",
    "def preprocess():\n",
    "    # Read train and test csv\n",
    "    train = pd.read_csv('./shopee-product-matching/train.csv')\n",
    "    test = pd.read_csv('./shopee-product-matching/test.csv')\n",
    "    # Drop duplicates images to avoid leakage (dont know if this is correct)\n",
    "    train.drop_duplicates(subset = ['image'], inplace = True)\n",
    "    train.reset_index(drop = True, inplace = True)\n",
    "    label_mapper = dict(zip(train['label_group'].unique(), np.arange(len(train['label_group'].unique()))))\n",
    "    label_mapper_inv = dict(zip(np.arange(len(train['label_group'].unique())), train['label_group'].unique()))\n",
    "    train['label_group'] = train['label_group'].map(label_mapper)\n",
    "    # Number of classes\n",
    "    N_CLASSES = train['label_group'].nunique()\n",
    "    # Get ground truth labels format\n",
    "    tmp = train.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "    train['matches'] = train['label_group'].map(tmp)\n",
    "    train['matches'] = train['matches'].apply(lambda x: ' '.join(x))\n",
    "    ground_truth = train[['posting_id', 'matches']]\n",
    "    # Calculate naive score using self-post\n",
    "    ground_truth['f1'] = f1_score(ground_truth['matches'], ground_truth['posting_id'])\n",
    "    score = ground_truth['f1'].mean()\n",
    "    print(f'Using the same posting id as prediction our f1 score is {score}')\n",
    "    return train, test, label_mapper, label_mapper_inv, N_CLASSES, ground_truth\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Function to decode our images\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to read our image and return image, label_group\n",
    "def read_image(image, label_group):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = decode_image(image)\n",
    "    return image, label_group\n",
    "\n",
    "# Function to get our training dataset\n",
    "def get_training_dataset(image, label_group):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image, label_group))\n",
    "    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "# Function to get our validation dataset\n",
    "def get_validation_dataset(image, label_group):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image, label_group))\n",
    "    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "    \n",
    "# Function to split our data into train and validation\n",
    "def train_and_eval_split(image, label_group):\n",
    "    trn_image, val_image, trn_labels, val_labels = train_test_split(image, label_group, random_state = SEED, shuffle = True)\n",
    "    return trn_image, val_image, trn_labels, val_labels\n",
    "\n",
    "# Function to create our EfficientNetB0 model\n",
    "def get_model():\n",
    "        \n",
    "    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3))\n",
    "    x = efn.EfficientNetB0(include_top = False, weights = 'imagenet')(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    output = tf.keras.layers.Dense(N_CLASSES, activation = 'softmax')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [inp], outputs = [output])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function for a custom learning rate scheduler with warmup and decay\n",
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n",
    "    return lr_callback\n",
    "\n",
    "# Function to train and evaluate our model\n",
    "def train_and_evaluate(image, label_group):\n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    # Seed everything\n",
    "    seed_everything(SEED)\n",
    "    STEPS_PER_EPOCH = len(image) // BATCH_SIZE\n",
    "    K.clear_session()\n",
    "    model = get_model()\n",
    "    image = './shopee-product-matching/train_images/' + image\n",
    "    trn_image, val_image, trn_labels, val_labels = train_and_eval_split(image, label_group)\n",
    "    train_dataset = get_training_dataset(trn_image, trn_labels)\n",
    "    val_dataset = get_validation_dataset(val_image, val_labels)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB0_{IMAGE_SIZE[0]}_{SEED}.h5', \n",
    "                                                    monitor = 'val_loss', \n",
    "                                                    verbose = VERBOSE, \n",
    "                                                    save_best_only = True,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min')\n",
    "    history = model.fit(train_dataset,\n",
    "                        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                        epochs = EPOCHS,\n",
    "                        callbacks = [checkpoint, get_lr_callback()], \n",
    "                        validation_data = val_dataset,\n",
    "                        verbose = VERBOSE)\n",
    "    \n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    print('Training Complete...')\n",
    "    \n",
    "    return model, val_image\n",
    "\n",
    "def get_cv_score(image, label_group, model, val_image):\n",
    "    \n",
    "    model.load_weights(f'EfficientNetB0_{IMAGE_SIZE[0]}_{SEED}.h5')\n",
    "    model = tf.keras.models.Model(inputs = model.input, outputs = model.layers[-2].output)\n",
    "    \n",
    "    # Respect order\n",
    "    image = './shopee-product-matching/train_images/' + image\n",
    "    dataset_images = get_validation_dataset(image, label_group)\n",
    "    dataset_images = dataset_images.map(lambda image, label_group: image)\n",
    "    # Predict the entire dataset\n",
    "    embeddings = model.predict(dataset_images)\n",
    "    \n",
    "    # Find the best threshold (lazy optimization)\n",
    "    predictions_08 = []\n",
    "    predictions_09 = []\n",
    "    predictions_10 = []\n",
    "    predictions_11 = []\n",
    "    predictions_12 = []\n",
    "    predictions_13 = []\n",
    "    predictions_14 = []\n",
    "    predictions_15 = []\n",
    "    predictions_16 = []\n",
    "    # Iterate over each validation image and use cosine distance to find similar images\n",
    "    for val_index in tqdm(val_image.index):\n",
    "        distances = spatial.distance.cdist(\n",
    "            embeddings[np.newaxis, val_index, :], embeddings, 'cosine')[0]\n",
    "        # Only get small distances\n",
    "        TOP_08 = len(distances[distances <= 0.08])\n",
    "        TOP_09 = len(distances[distances <= 0.09])\n",
    "        TOP_10 = len(distances[distances <= 0.10])\n",
    "        TOP_11 = len(distances[distances <= 0.11])\n",
    "        TOP_12 = len(distances[distances <= 0.12])\n",
    "        TOP_13 = len(distances[distances <= 0.13])\n",
    "        TOP_14 = len(distances[distances <= 0.14])\n",
    "        TOP_15 = len(distances[distances <= 0.15])\n",
    "        TOP_16 = len(distances[distances <= 0.16])\n",
    "        top_k_08 = list(np.argsort(distances)[:TOP_08])\n",
    "        top_k_09 = list(np.argsort(distances)[:TOP_09])\n",
    "        top_k_10 = list(np.argsort(distances)[:TOP_10])\n",
    "        top_k_11 = list(np.argsort(distances)[:TOP_11])\n",
    "        top_k_12 = list(np.argsort(distances)[:TOP_12])\n",
    "        top_k_13 = list(np.argsort(distances)[:TOP_13])\n",
    "        top_k_14 = list(np.argsort(distances)[:TOP_14])\n",
    "        top_k_15 = list(np.argsort(distances)[:TOP_15])\n",
    "        top_k_16 = list(np.argsort(distances)[:TOP_16])\n",
    "        predictions_08.append(' '.join(train['posting_id'].iloc[top_k_08].values))\n",
    "        predictions_09.append(' '.join(train['posting_id'].iloc[top_k_09].values))\n",
    "        predictions_10.append(' '.join(train['posting_id'].iloc[top_k_10].values))\n",
    "        predictions_11.append(' '.join(train['posting_id'].iloc[top_k_11].values))\n",
    "        predictions_12.append(' '.join(train['posting_id'].iloc[top_k_12].values))\n",
    "        predictions_13.append(' '.join(train['posting_id'].iloc[top_k_13].values))\n",
    "        predictions_14.append(' '.join(train['posting_id'].iloc[top_k_14].values))\n",
    "        predictions_15.append(' '.join(train['posting_id'].iloc[top_k_15].values))\n",
    "        predictions_16.append(' '.join(train['posting_id'].iloc[top_k_16].values))\n",
    "\n",
    "    val_predictions = ground_truth.loc[val_image.index]\n",
    "    val_predictions['predictions_08'] = predictions_08\n",
    "    val_predictions['predictions_09'] = predictions_09\n",
    "    val_predictions['predictions_10'] = predictions_10\n",
    "    val_predictions['predictions_11'] = predictions_11\n",
    "    val_predictions['predictions_12'] = predictions_12\n",
    "    val_predictions['predictions_13'] = predictions_13\n",
    "    val_predictions['predictions_14'] = predictions_14\n",
    "    val_predictions['predictions_15'] = predictions_15\n",
    "    val_predictions['predictions_16'] = predictions_16\n",
    "    val_predictions['f1_08'] = f1_score(val_predictions['matches'], val_predictions['predictions_08'])\n",
    "    val_predictions['f1_09'] = f1_score(val_predictions['matches'], val_predictions['predictions_09'])\n",
    "    val_predictions['f1_10'] = f1_score(val_predictions['matches'], val_predictions['predictions_10'])\n",
    "    val_predictions['f1_11'] = f1_score(val_predictions['matches'], val_predictions['predictions_11'])\n",
    "    val_predictions['f1_12'] = f1_score(val_predictions['matches'], val_predictions['predictions_12'])\n",
    "    val_predictions['f1_13'] = f1_score(val_predictions['matches'], val_predictions['predictions_13'])\n",
    "    val_predictions['f1_14'] = f1_score(val_predictions['matches'], val_predictions['predictions_14'])\n",
    "    val_predictions['f1_15'] = f1_score(val_predictions['matches'], val_predictions['predictions_15'])\n",
    "    val_predictions['f1_16'] = f1_score(val_predictions['matches'], val_predictions['predictions_16'])\n",
    "    print('Our f1 score with threshold 0.08 for the validation set is {}'.format(val_predictions['f1_08'].mean()))\n",
    "    print('Our f1 score with threshold 0.09 for the validation set is {}'.format(val_predictions['f1_09'].mean()))\n",
    "    print('Our f1 score with threshold 0.10 for the validation set is {}'.format(val_predictions['f1_10'].mean()))\n",
    "    print('Our f1 score with threshold 0.11 for the validation set is {}'.format(val_predictions['f1_11'].mean()))\n",
    "    print('Our f1 score with threshold 0.12 for the validation set is {}'.format(val_predictions['f1_12'].mean()))\n",
    "    print('Our f1 score with threshold 0.13 for the validation set is {}'.format(val_predictions['f1_13'].mean()))\n",
    "    print('Our f1 score with threshold 0.14 for the validation set is {}'.format(val_predictions['f1_14'].mean()))\n",
    "    print('Our f1 score with threshold 0.15 for the validation set is {}'.format(val_predictions['f1_15'].mean()))\n",
    "    print('Our f1 score with threshold 0.16 for the validation set is {}'.format(val_predictions['f1_16'].mean()))\n",
    "    return val_predictions\n",
    "\n",
    "train, test, label_mapper, label_mapper_inv, N_CLASSES, ground_truth = preprocess()\n",
    "model, val_image = train_and_evaluate(train['image'], train['label_group'])\n",
    "val_predictions = get_cv_score(train['image'], train['label_group'], model, val_image)\n",
    "val_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
